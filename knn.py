# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/177URgi9s1UFvT8pP1pIN0o1m7HhRo2SV
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('ggplot')
from sklearn import neighbors
from sklearn.metrics import auc, roc_curve, accuracy_score
from sklearn import tree
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
import seaborn as sns
# Import and suppress warnings
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("winequality-white.csv", sep = ";")
print(df.isnull().head()) # check for missing values (This is part of our CW req)

corr = df.corr() 
# import seaborn as sns
plt.figure(figsize = (10,10))
sns.heatmap(corr, vmax = .8, linewidths = 0.01, square = True, annot = True, cmap = 'RdYlGn', linecolor = 'white')
plt.show()

# assigning the independent  variables to X
X = df.loc[:, 'fixed acidity':'alcohol']
feature_names = X.columns.values 

# converting the dependent variable from numeric to categorical
def score_to_label(x):
    if x > 5:
        return 1
    else:
        return 0
        
# replacing the numeric 'quality' with categorical 'label'
df.quality = df.quality.apply(score_to_label)
df.quality, class_names = pd.factorize(df.quality)
y = df.quality

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 100)

# scaling the numeric attributes
# from sklearn.preprocessingimport StandardScaler
scaler = StandardScaler().fit(X_train)
# scaled X
X_train = scaler.transform(X_train) 
X_test = scaler.transform(X_test)

# k-nearest neighbors
knn_clf = neighbors.KNeighborsClassifier(10, weights = "distance")
knn_clf.fit(X_train, y_train)
knn_y_pred = knn_clf.predict(X_test)

# metrics and ROC curve
knn_acc = accuracy_score(y_test, knn_y_pred)
print(knn_acc)